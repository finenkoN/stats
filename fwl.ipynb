{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1015349,"sourceType":"datasetVersion","datasetId":426888},{"sourceId":13322988,"sourceType":"datasetVersion","datasetId":8446244}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"aebc3374","cell_type":"markdown","source":"# Задача 1\n\n## Про FWL-Теорему\n","metadata":{}},{"id":"1c523447","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n \nnp.random.seed(42069)\n \n# Пусть у нас есть набор данных, где есть линейная зависимость y от X1 и X2\n# При этом X1 и X2 тоже малясь зависимые\ndf = pd.DataFrame({'x1': np.random.uniform(0, 10, size=1000)})\ndf['x2'] = 4.9 + df['x1'] * 0.983 + 2.104 * np.random.normal(0, 1.35, size=1000)\ndf['y'] = 8.643 - 2.34 * df['x1'] + 3.35 * df['x2'] + np.random.normal(0, 1.65, size=1000)\ndf['const'] = 1\n \n# Построим линейную регрессию-МНК из 1, X1 и X2\nmodel = sm.OLS(\n    endog=df['y'],\n    exog=df[['const', 'x1', 'x2']]\n).fit()\n\n# Внимательно смотрим на коэффициент при X2\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T13:44:59.585766Z","iopub.execute_input":"2025-10-12T13:44:59.588482Z","iopub.status.idle":"2025-10-12T13:45:02.908066Z","shell.execute_reply.started":"2025-10-12T13:44:59.588438Z","shell.execute_reply":"2025-10-12T13:45:02.907156Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.972\nModel:                            OLS   Adj. R-squared:                  0.972\nMethod:                 Least Squares   F-statistic:                 1.754e+04\nDate:                Sun, 12 Oct 2025   Prob (F-statistic):               0.00\nTime:                        13:45:02   Log-Likelihood:                -1934.3\nNo. Observations:                1000   AIC:                             3875.\nDf Residuals:                     997   BIC:                             3889.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          8.6842      0.139     62.606      0.000       8.412       8.956\nx1            -2.3455      0.027    -88.274      0.000      -2.398      -2.293\nx2             3.3544      0.019    178.202      0.000       3.317       3.391\n==============================================================================\nOmnibus:                        0.497   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.780   Jarque-Bera (JB):                0.394\nSkew:                          -0.036   Prob(JB):                        0.821\nKurtosis:                       3.066   Cond. No.                         31.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.972</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.972</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.754e+04</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 12 Oct 2025</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>13:45:02</td>     <th>  Log-Likelihood:    </th> <td> -1934.3</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>   3875.</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   997</td>      <th>  BIC:               </th> <td>   3889.</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>    8.6842</td> <td>    0.139</td> <td>   62.606</td> <td> 0.000</td> <td>    8.412</td> <td>    8.956</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>   -2.3455</td> <td>    0.027</td> <td>  -88.274</td> <td> 0.000</td> <td>   -2.398</td> <td>   -2.293</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    3.3544</td> <td>    0.019</td> <td>  178.202</td> <td> 0.000</td> <td>    3.317</td> <td>    3.391</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.497</td> <th>  Durbin-Watson:     </th> <td>   2.031</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.780</td> <th>  Jarque-Bera (JB):  </th> <td>   0.394</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.036</td> <th>  Prob(JB):          </th> <td>   0.821</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 3.066</td> <th>  Cond. No.          </th> <td>    31.5</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/latex":"\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.972   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.972   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 1.754e+04   \\\\\n\\textbf{Date:}             & Sun, 12 Oct 2025 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n\\textbf{Time:}             &     13:45:02     & \\textbf{  Log-Likelihood:    } &   -1934.3   \\\\\n\\textbf{No. Observations:} &        1000      & \\textbf{  AIC:               } &     3875.   \\\\\n\\textbf{Df Residuals:}     &         997      & \\textbf{  BIC:               } &     3889.   \\\\\n\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{const} &       8.6842  &        0.139     &    62.606  &         0.000        &        8.412    &        8.956     \\\\\n\\textbf{x1}    &      -2.3455  &        0.027     &   -88.274  &         0.000        &       -2.398    &       -2.293     \\\\\n\\textbf{x2}    &       3.3544  &        0.019     &   178.202  &         0.000        &        3.317    &        3.391     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       &  0.497 & \\textbf{  Durbin-Watson:     } &    2.031  \\\\\n\\textbf{Prob(Omnibus):} &  0.780 & \\textbf{  Jarque-Bera (JB):  } &    0.394  \\\\\n\\textbf{Skew:}          & -0.036 & \\textbf{  Prob(JB):          } &    0.821  \\\\\n\\textbf{Kurtosis:}      &  3.066 & \\textbf{  Cond. No.          } &     31.5  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."},"metadata":{}}],"execution_count":1},{"id":"a4771c16","cell_type":"code","source":"# Научим регрессию X1 на X2\nmodel_x2 = sm.OLS(\n    endog=df['x2'],\n    exog=df[['const', 'x1']]\n).fit()\n\n# Научим регрессию X1 на y\nmodel_yx1 = sm.OLS(\n    endog=df['y'],\n    exog=df[['const', 'x1']]\n).fit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:19:18.120915Z","iopub.execute_input":"2025-10-12T10:19:18.121779Z","iopub.status.idle":"2025-10-12T10:19:18.130996Z","shell.execute_reply.started":"2025-10-12T10:19:18.121749Z","shell.execute_reply":"2025-10-12T10:19:18.130217Z"}},"outputs":[],"execution_count":3},{"id":"5a7fa51b","cell_type":"code","source":"# Полученными регрессиями \"предскажем X2 и Y\"\n\ndf['yx1'] = model_yx1.predict(df[['const', 'x1']])\ndf['x2x1'] = model_x2.predict(df[['const', 'x1']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:19:20.437178Z","iopub.execute_input":"2025-10-12T10:19:20.437481Z","iopub.status.idle":"2025-10-12T10:19:20.445459Z","shell.execute_reply.started":"2025-10-12T10:19:20.437459Z","shell.execute_reply":"2025-10-12T10:19:20.444628Z"}},"outputs":[],"execution_count":4},{"id":"eb8df89b","cell_type":"code","source":"# А затем \"отпилим\" предсказание из данных\ndf['y_detrended'] = df['y'] - df['yx1']\ndf['x2_detrended'] = df['x2'] - df['x2x1']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:19:22.680466Z","iopub.execute_input":"2025-10-12T10:19:22.680769Z","iopub.status.idle":"2025-10-12T10:19:22.686697Z","shell.execute_reply.started":"2025-10-12T10:19:22.680746Z","shell.execute_reply":"2025-10-12T10:19:22.685725Z"}},"outputs":[],"execution_count":5},{"id":"3b26f03a","cell_type":"code","source":"# Учим модель на \"очищенных\" переменных и, о Боже, коэффициент при X2 остается \"как был\"\nmodel_detrended = sm.OLS(\n    endog=df['y_detrended'],\n    exog=df[['const', 'x2_detrended']]\n).fit()\nmodel_detrended.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T10:19:24.728211Z","iopub.execute_input":"2025-10-12T10:19:24.728522Z","iopub.status.idle":"2025-10-12T10:19:24.755601Z","shell.execute_reply.started":"2025-10-12T10:19:24.728499Z","shell.execute_reply":"2025-10-12T10:19:24.754870Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            y_detrended   R-squared:                       0.970\nModel:                            OLS   Adj. R-squared:                  0.970\nMethod:                 Least Squares   F-statistic:                 3.179e+04\nDate:                Sun, 12 Oct 2025   Prob (F-statistic):               0.00\nTime:                        10:19:24   Log-Likelihood:                -1934.3\nNo. Observations:                1000   AIC:                             3873.\nDf Residuals:                     998   BIC:                             3882.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst         4.469e-15      0.053   8.43e-14      1.000      -0.104       0.104\nx2_detrended     3.3544      0.019    178.292      0.000       3.318       3.391\n==============================================================================\nOmnibus:                        0.497   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.780   Jarque-Bera (JB):                0.394\nSkew:                          -0.036   Prob(JB):                        0.821\nKurtosis:                       3.066   Cond. No.                         2.82\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>       <td>y_detrended</td>   <th>  R-squared:         </th> <td>   0.970</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.970</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.179e+04</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 12 Oct 2025</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>10:19:24</td>     <th>  Log-Likelihood:    </th> <td> -1934.3</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>   3873.</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   998</td>      <th>  BIC:               </th> <td>   3882.</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>        <td> 4.469e-15</td> <td>    0.053</td> <td> 8.43e-14</td> <td> 1.000</td> <td>   -0.104</td> <td>    0.104</td>\n</tr>\n<tr>\n  <th>x2_detrended</th> <td>    3.3544</td> <td>    0.019</td> <td>  178.292</td> <td> 0.000</td> <td>    3.318</td> <td>    3.391</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.497</td> <th>  Durbin-Watson:     </th> <td>   2.031</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.780</td> <th>  Jarque-Bera (JB):  </th> <td>   0.394</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.036</td> <th>  Prob(JB):          </th> <td>   0.821</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 3.066</td> <th>  Cond. No.          </th> <td>    2.82</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/latex":"\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &   y\\_detrended   & \\textbf{  R-squared:         } &     0.970   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.970   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 3.179e+04   \\\\\n\\textbf{Date:}             & Sun, 12 Oct 2025 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n\\textbf{Time:}             &     10:19:24     & \\textbf{  Log-Likelihood:    } &   -1934.3   \\\\\n\\textbf{No. Observations:} &        1000      & \\textbf{  AIC:               } &     3873.   \\\\\n\\textbf{Df Residuals:}     &         998      & \\textbf{  BIC:               } &     3882.   \\\\\n\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{const}         &    4.469e-15  &        0.053     &  8.43e-14  &         1.000        &       -0.104    &        0.104     \\\\\n\\textbf{x2\\_detrended} &       3.3544  &        0.019     &   178.292  &         0.000        &        3.318    &        3.391     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       &  0.497 & \\textbf{  Durbin-Watson:     } &    2.031  \\\\\n\\textbf{Prob(Omnibus):} &  0.780 & \\textbf{  Jarque-Bera (JB):  } &    0.394  \\\\\n\\textbf{Skew:}          & -0.036 & \\textbf{  Prob(JB):          } &    0.821  \\\\\n\\textbf{Kurtosis:}      &  3.066 & \\textbf{  Cond. No.          } &     2.82  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."},"metadata":{}}],"execution_count":6},{"id":"e7e39744","cell_type":"markdown","source":"Вроде коэффициент тот же. А теперь задача:\n\nВозьмите данные c kaggle, например [отсюда](https://www.kaggle.com/code/malakalaabiad/house-prices-techniques/input) и удостоверьтесь, что FWL-теорема работает для случая нескольких переменных.","metadata":{}},{"id":"4bde1259-8094-479d-83a7-7098a7816125","cell_type":"code","source":"from statsmodels.formula.api import ols\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf_house = pd.read_csv('/kaggle/input/house-prices/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:03:04.989699Z","iopub.execute_input":"2025-10-12T11:03:04.989988Z","iopub.status.idle":"2025-10-12T11:03:05.042658Z","shell.execute_reply.started":"2025-10-12T11:03:04.989967Z","shell.execute_reply":"2025-10-12T11:03:05.041646Z"}},"outputs":[],"execution_count":9},{"id":"8c5f4752-08c6-47fb-86f3-ee87eebe13d4","cell_type":"code","source":"selected_features = ['SalePrice', 'GrLivArea', 'GarageArea', 'OverallQual', 'YearBuilt', 'TotalBsmtSF']\ndf_house_clean = df_house[selected_features].dropna()\ndf_house_clean['const'] = 1\nexog_full = ['GrLivArea', 'GarageArea', 'OverallQual', 'YearBuilt', 'TotalBsmtSF']\nformula_full = 'SalePrice ~ ' + ' + '.join(exog_full)\n\nmodel_full = ols(formula_full, data=df_house_clean).fit()\ncoef_full = model_full.params\nmodel_full.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:18:07.060839Z","iopub.execute_input":"2025-10-12T11:18:07.061194Z","iopub.status.idle":"2025-10-12T11:18:07.109959Z","shell.execute_reply.started":"2025-10-12T11:18:07.061170Z","shell.execute_reply":"2025-10-12T11:18:07.109011Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              SalePrice   R-squared:                       0.767\nModel:                            OLS   Adj. R-squared:                  0.766\nMethod:                 Least Squares   F-statistic:                     955.8\nDate:                Sun, 12 Oct 2025   Prob (F-statistic):               0.00\nTime:                        11:18:07   Log-Likelihood:                -17481.\nNo. Observations:                1460   AIC:                         3.497e+04\nDf Residuals:                    1454   BIC:                         3.501e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -7.355e+05   8.31e+04     -8.850      0.000   -8.98e+05   -5.72e+05\nGrLivArea      51.2661      2.567     19.973      0.000      46.231      56.301\nGarageArea     45.9846      6.187      7.432      0.000      33.847      58.122\nOverallQual  2.099e+04   1148.201     18.277      0.000    1.87e+04    2.32e+04\nYearBuilt     334.6471     43.687      7.660      0.000     248.951     420.344\nTotalBsmtSF    27.6736      2.874      9.631      0.000      22.037      33.310\n==============================================================================\nOmnibus:                      511.356   Durbin-Watson:                   1.981\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            62487.893\nSkew:                          -0.561   Prob(JB):                         0.00\nKurtosis:                      35.030   Cond. No.                     2.29e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.29e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>        <td>SalePrice</td>    <th>  R-squared:         </th> <td>   0.767</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.766</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   955.8</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 12 Oct 2025</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>11:18:07</td>     <th>  Log-Likelihood:    </th> <td> -17481.</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>3.497e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  1454</td>      <th>  BIC:               </th> <td>3.501e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>   <td>-7.355e+05</td> <td> 8.31e+04</td> <td>   -8.850</td> <td> 0.000</td> <td>-8.98e+05</td> <td>-5.72e+05</td>\n</tr>\n<tr>\n  <th>GrLivArea</th>   <td>   51.2661</td> <td>    2.567</td> <td>   19.973</td> <td> 0.000</td> <td>   46.231</td> <td>   56.301</td>\n</tr>\n<tr>\n  <th>GarageArea</th>  <td>   45.9846</td> <td>    6.187</td> <td>    7.432</td> <td> 0.000</td> <td>   33.847</td> <td>   58.122</td>\n</tr>\n<tr>\n  <th>OverallQual</th> <td> 2.099e+04</td> <td> 1148.201</td> <td>   18.277</td> <td> 0.000</td> <td> 1.87e+04</td> <td> 2.32e+04</td>\n</tr>\n<tr>\n  <th>YearBuilt</th>   <td>  334.6471</td> <td>   43.687</td> <td>    7.660</td> <td> 0.000</td> <td>  248.951</td> <td>  420.344</td>\n</tr>\n<tr>\n  <th>TotalBsmtSF</th> <td>   27.6736</td> <td>    2.874</td> <td>    9.631</td> <td> 0.000</td> <td>   22.037</td> <td>   33.310</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>511.356</td> <th>  Durbin-Watson:     </th> <td>   1.981</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>62487.893</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.561</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td>35.030</td>  <th>  Cond. No.          </th> <td>2.29e+05</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.29e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems.","text/latex":"\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &    SalePrice     & \\textbf{  R-squared:         } &     0.767   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.766   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     955.8   \\\\\n\\textbf{Date:}             & Sun, 12 Oct 2025 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n\\textbf{Time:}             &     11:18:07     & \\textbf{  Log-Likelihood:    } &   -17481.   \\\\\n\\textbf{No. Observations:} &        1460      & \\textbf{  AIC:               } & 3.497e+04   \\\\\n\\textbf{Df Residuals:}     &        1454      & \\textbf{  BIC:               } & 3.501e+04   \\\\\n\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                     & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept}   &   -7.355e+05  &     8.31e+04     &    -8.850  &         0.000        &    -8.98e+05    &    -5.72e+05     \\\\\n\\textbf{GrLivArea}   &      51.2661  &        2.567     &    19.973  &         0.000        &       46.231    &       56.301     \\\\\n\\textbf{GarageArea}  &      45.9846  &        6.187     &     7.432  &         0.000        &       33.847    &       58.122     \\\\\n\\textbf{OverallQual} &    2.099e+04  &     1148.201     &    18.277  &         0.000        &     1.87e+04    &     2.32e+04     \\\\\n\\textbf{YearBuilt}   &     334.6471  &       43.687     &     7.660  &         0.000        &      248.951    &      420.344     \\\\\n\\textbf{TotalBsmtSF} &      27.6736  &        2.874     &     9.631  &         0.000        &       22.037    &       33.310     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 511.356 & \\textbf{  Durbin-Watson:     } &     1.981  \\\\\n\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 62487.893  \\\\\n\\textbf{Skew:}          &  -0.561 & \\textbf{  Prob(JB):          } &      0.00  \\\\\n\\textbf{Kurtosis:}      &  35.030 & \\textbf{  Cond. No.          } &  2.29e+05  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n [2] The condition number is large, 2.29e+05. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."},"metadata":{}}],"execution_count":18},{"id":"10282fee-0274-4bf6-8a2d-c55dc7e1836e","cell_type":"code","source":"variable_to_check = 'GarageArea'\nother_exog = [f for f in exog_full if f != variable_to_check]\n\nformula_y_on_others = f'SalePrice ~ ' + ' + '.join(other_exog)\nmodel_y_on_others = ols(formula_y_on_others, data=df_house_clean).fit()\ndf_house_clean['res_y'] = model_y_on_others.resid\nformula_xk_on_others = f'{variable_to_check} ~ ' + ' + '.join(other_exog)\nmodel_xk_on_others = ols(formula_xk_on_others, data=df_house_clean).fit()\ndf_house_clean['res_xk'] = model_xk_on_others.resid\n\nmodel_fwl = sm.OLS(\n        endog=df_house_clean['res_y'],\n        exog=df_house_clean[['const', 'res_xk']]\n    ).fit()\n# Сравнение коэффициентов\ncoef_fwl = model_fwl.params['res_xk']\nprint(f\"\\nКоэффициент при '{variable_to_check}' из полной регрессии: {coef_full[variable_to_check]:.4f}\")\nprint(f\"Коэффициент при 'res_{variable_to_check}' из FWL-регрессии: {coef_fwl:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:14:39.490814Z","iopub.execute_input":"2025-10-12T11:14:39.491179Z","iopub.status.idle":"2025-10-12T11:14:39.525530Z","shell.execute_reply.started":"2025-10-12T11:14:39.491153Z","shell.execute_reply":"2025-10-12T11:14:39.524699Z"}},"outputs":[{"name":"stdout","text":"\nКоэффициент при 'GarageArea' из полной регрессии: 45.9846\nКоэффициент при 'res_GarageArea' из FWL-регрессии: 45.9846\n","output_type":"stream"}],"execution_count":16},{"id":"6df1d06c-3994-4d13-be87-2b16092443dd","cell_type":"code","source":"variable_to_check_2 = 'OverallQual'\nother_exog_2 = [f for f in exog_full if f != variable_to_check_2]\n\nmodel_y_on_others_2 = ols(f'SalePrice ~ ' + ' + '.join(other_exog_2), data=df_house_clean).fit()\ndf_house_clean['res_y_2'] = model_y_on_others_2.resid\n\nmodel_xk_on_others_2 = ols(f'{variable_to_check_2} ~ ' + ' + '.join(other_exog_2), data=df_house_clean).fit()\ndf_house_clean['res_xk_2'] = model_xk_on_others_2.resid\nmodel_fwl_2 = sm.OLS(\n        endog=df_house_clean['res_y_2'],\n        exog=df_house_clean[['const', 'res_xk_2']]\n    ).fit()\n\ncoef_fwl_2 = model_fwl_2.params['res_xk_2']\nprint(f\"\\nКоэффициент при '{variable_to_check_2}' из полной регрессии: {coef_full[variable_to_check_2]:.4f}\")\nprint(f\"Коэффициент при 'res_{variable_to_check_2}' из FWL-регрессии: {coef_fwl_2:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:19:50.775960Z","iopub.execute_input":"2025-10-12T11:19:50.776547Z","iopub.status.idle":"2025-10-12T11:19:50.810525Z","shell.execute_reply.started":"2025-10-12T11:19:50.776522Z","shell.execute_reply":"2025-10-12T11:19:50.809652Z"}},"outputs":[{"name":"stdout","text":"\nКоэффициент при 'OverallQual' из полной регрессии: 20985.4055\nКоэффициент при 'res_OverallQual' из FWL-регрессии: 20985.4055\n","output_type":"stream"}],"execution_count":20},{"id":"54cc4f18","cell_type":"markdown","source":"# Задача 2\n\nВозьмите [данные](https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop) с Kaggle и оцените равномерность разбиения их на группы (для будущего АБ-теста) с помощью различных видов хеширования:\n\n1. md5\n2. sha256\n3. Улучшится ли равномерность, если вместо одинарного использования md5 применить [вот такую](https://towardsdatascience.com/assign-experiment-variants-at-scale-in-a-b-tests-e80fedb2779d) двухуровневую процедуру с тем же md5, проверить на тех же данных","metadata":{}},{"id":"2f7a5bd7","cell_type":"code","source":"import kagglehub\nimport hashlib\nfrom scipy.stats import chi2\nfrom tqdm.notebook import tqdm, trange\npath = kagglehub.dataset_download(\"mkechinov/ecommerce-events-history-in-cosmetics-shop\")\ndf = pd.concat([pd.read_csv(path + \"/2019-Dec.csv\"),\n                  pd.read_csv(path + \"/2019-Nov.csv\"),\n                  pd.read_csv(path +\"/2019-Oct.csv\"),\n                  pd.read_csv(path +\"/2020-Feb.csv\"),\n                  pd.read_csv(path +\"/2020-Jan.csv\")])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T13:56:00.318627Z","iopub.execute_input":"2025-10-12T13:56:00.319683Z","iopub.status.idle":"2025-10-12T13:56:52.777998Z","shell.execute_reply.started":"2025-10-12T13:56:00.319646Z","shell.execute_reply":"2025-10-12T13:56:52.776799Z"}},"outputs":[],"execution_count":11},{"id":"722ec570-0f51-474b-b0a9-56da70ff781c","cell_type":"code","source":"user_ids_raw = df['user_id'].dropna().astype(str).unique()\nuser_ids_array = user_ids_raw.astype(object)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T13:57:21.851369Z","iopub.execute_input":"2025-10-12T13:57:21.851843Z","iopub.status.idle":"2025-10-12T13:57:31.568096Z","shell.execute_reply.started":"2025-10-12T13:57:21.851814Z","shell.execute_reply":"2025-10-12T13:57:31.566821Z"}},"outputs":[],"execution_count":12},{"id":"b7786491-0bbe-4fc9-b9c0-0f8fd3290bcc","cell_type":"code","source":"class HashEvaluator:\n    def __init__(self, method_name: str, base_salt: str = \"A_B_TEST_SALT\", traffic_split_ratio: float = 0.0):\n        if method_name == 'md5':\n            self._hash_generator = self._generate_md5_hash\n        elif method_name == 'sha256':\n            self._hash_generator = self._generate_sha256_hash\n        elif method_name == 'two_level_md5':\n            self._hash_generator = self._generate_two_level_md5\n        else:\n            raise ValueError(\"Unsupported hash method provided.\")\n        \n        self.salt_value = base_salt\n        self.split_ratio = traffic_split_ratio # Для имитации отфильтровывания трафика, если нужно\n\n    # Статический метод для MD5\n    @staticmethod\n    def _generate_md5_hash(input_string: str) -> int:\n        return int(hashlib.md5(input_string.encode()).hexdigest(), 16)\n\n    # Статический метод для SHA256\n    @staticmethod\n    def _generate_sha256_hash(input_string: str) -> int:\n        return int(hashlib.sha256(input_string.encode()).hexdigest(), 16)\n\n    # Двухуровневый MD5\n    def _generate_two_level_md5(self, input_string: str, n_buckets: int) -> int:\n        first_level_hash = self._generate_md5_hash(input_string)\n        \n        second_level_input = str(first_level_hash) + self.salt_value # Второй соляной компонент\n        final_hash_value = self._generate_md5_hash(second_level_input)\n        \n        return final_hash_value % n_buckets\n\n    def _assign_to_buckets(self, user_ids: np.ndarray, num_buckets: int, test_layer_id: str) -> np.ndarray:\n        bucket_counts = np.zeros(num_buckets, dtype=np.int64)\n        for user_id_str in user_ids:\n            if self._hash_generator == self._generate_two_level_md5:\n                bucket_index = self._hash_generator(str(user_id_str) + test_layer_id + self.salt_value, num_buckets)\n            else:\n                bucket_index = self._hash_generator(str(user_id_str) + test_layer_id + self.salt_value) % num_buckets\n            \n            \n            bucket_counts[bucket_index] += 1\n        return bucket_counts\n\n    @staticmethod\n    def _perform_chi_squared_test(counts: np.ndarray, significance_level: float = 0.05) -> bool:\n        k_buckets = counts.size\n        total_samples = counts.sum()\n        expected_counts = np.full(k_buckets, total_samples / k_buckets)\n        \n        if total_samples == 0:\n            return False # Нельзя провести тест\n            \n        chi2_statistic = ((counts - expected_counts) ** 2 / expected_counts).sum()\n        p_val = 1 - chi2.cdf(chi2_statistic, k_buckets - 1)\n        return p_val < significance_level # Возвращаем True, если обнаружена неравномерность (p < alpha)\n\n    def calculate_false_positive_rate(self, user_set: np.ndarray, num_buckets: int, num_simulations: int, alpha: float = 0.05) -> float:\n        false_positives = 0\n        for _ in tqdm(range(num_simulations), desc=\"False Positive Rate\"):\n            # Генерируем новый уникальный layer_id для каждой симуляции\n            current_layer_id = f\"fp_layer_{np.random.randint(1000000)}\"\n            counts = self._assign_to_buckets(user_set, num_buckets, current_layer_id)\n            if self._perform_chi_squared_test(counts, alpha):\n                false_positives += 1\n        return false_positives / num_simulations\n\n    def measure_srm_sensitivity(self, user_set: np.ndarray, num_buckets: int, num_simulations: int, srm_step: float = 0.001, max_srm_deviation: float = 0.1, alpha: float = 0.05) -> tuple[float, float]:\n        detected_srm_values = np.zeros(num_simulations)\n        \n        for i_sim in trange(num_simulations, desc=\"SRM Sensitivity\"):\n            current_layer_id = f\"srm_layer_{np.random.randint(1000000)}\"\n            initial_counts = self._assign_to_buckets(user_set, num_buckets, current_layer_id)\n            \n            simulated_srm = 0.0\n            modified_counts = np.copy(initial_counts)\n            \n            while simulated_srm <= max_srm_deviation:\n                added_users_for_srm = int(initial_counts.sum() * srm_step)\n                if simulated_srm > 0: # Только если SRM не 0\n                     modified_counts[0] += added_users_for_srm\n                \n                if self._perform_chi_squared_test(modified_counts, alpha):\n                    break\n                simulated_srm += srm_step\n            detected_srm_values[i_sim] = simulated_srm\n            \n        mean_srm = detected_srm_values.mean()\n        std_srm = detected_srm_values.std()\n        return mean_srm, std_srm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T13:57:34.446297Z","iopub.execute_input":"2025-10-12T13:57:34.447155Z","iopub.status.idle":"2025-10-12T13:57:34.463243Z","shell.execute_reply.started":"2025-10-12T13:57:34.447126Z","shell.execute_reply":"2025-10-12T13:57:34.462222Z"}},"outputs":[],"execution_count":13},{"id":"fe93df09-437b-4952-a9e3-435a74088923","cell_type":"code","source":"num_test_buckets = 100 # Количество бакетов для хеширования\nnum_checks_simulations = 50 # Количество симуляций для FPR и SRM\n\nanalysis_results = []\nhash_methods = ['md5', 'sha256', 'two_level_md5']\nmethod_display_names = {\n    'md5': 'MD5',\n    'sha256': 'SHA256',\n    'two_level_md5': 'Two-level MD5'\n}\n\nfor method in hash_methods:\n    evaluator = HashEvaluator(method, base_salt=\"MY_UNIQUE_SALT_FOR_TEST\")\n    fpr = evaluator.calculate_false_positive_rate(user_ids_array, num_test_buckets, num_checks_simulations)\n    srm_mean, srm_std = evaluator.measure_srm_sensitivity(user_ids_array, num_test_buckets, num_checks_simulations)\n    \n    analysis_results.append({\n        'Hash Method': method_display_names[method],\n        'False Positive Rate': f\"{fpr:.4f}\",\n        'SRM Sensitivity Mean': f\"{srm_mean:.4f}\",\n        'SRM Sensitivity Std': f\"{srm_std:.4f}\"\n    })\n\nresults_df_final = pd.DataFrame(analysis_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T13:57:39.587312Z","iopub.execute_input":"2025-10-12T13:57:39.587735Z","iopub.status.idle":"2025-10-12T14:18:58.880076Z","shell.execute_reply.started":"2025-10-12T13:57:39.587710Z","shell.execute_reply":"2025-10-12T14:18:58.878933Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"False Positive Rate:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"787538ad4d6a439991dbc7950c57c67b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"SRM Sensitivity:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4244881558d4934beb76251ae0410f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"False Positive Rate:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c740508f471e4928915e9898df232ff3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"SRM Sensitivity:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b4477b86ef043b7b31351ffded9f230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"False Positive Rate:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2e5b40dac024d72b77ecb94b1feeae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"SRM Sensitivity:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cf8fc1fa474faa893d52888fbae319"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'  Hash Method False Positive Rate SRM Sensitivity Mean SRM Sensitivity Std\\n          MD5              0.0400               0.0010              0.0002\\n       SHA256              0.0400               0.0009              0.0002\\nTwo-level MD5              0.0400               0.0009              0.0003'"},"metadata":{}}],"execution_count":14},{"id":"fe579fe3-7169-48ff-9766-582753211822","cell_type":"code","source":"results_df_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T14:23:04.813873Z","iopub.execute_input":"2025-10-12T14:23:04.814314Z","iopub.status.idle":"2025-10-12T14:23:04.836579Z","shell.execute_reply.started":"2025-10-12T14:23:04.814284Z","shell.execute_reply":"2025-10-12T14:23:04.835620Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"     Hash Method False Positive Rate SRM Sensitivity Mean SRM Sensitivity Std\n0            MD5              0.0400               0.0010              0.0002\n1         SHA256              0.0400               0.0009              0.0002\n2  Two-level MD5              0.0400               0.0009              0.0003","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Hash Method</th>\n      <th>False Positive Rate</th>\n      <th>SRM Sensitivity Mean</th>\n      <th>SRM Sensitivity Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MD5</td>\n      <td>0.0400</td>\n      <td>0.0010</td>\n      <td>0.0002</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SHA256</td>\n      <td>0.0400</td>\n      <td>0.0009</td>\n      <td>0.0002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Two-level MD5</td>\n      <td>0.0400</td>\n      <td>0.0009</td>\n      <td>0.0003</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"id":"02cc1c4d-e8ee-4046-8025-80dd47481429","cell_type":"markdown","source":"**Вывод**: Равномерность разбиения улучшается при переходе с MD5 на двухуровневый MD5, но не настолько, чтобы выбрать его вместо более простого SHA256","metadata":{}},{"id":"e998aac6","cell_type":"markdown","source":"# Задача 3\n\nПро эквивалентность или не эквивалентность разных методов подсчета квантилей\n\nСгенерируйте 2 выборки длины ,например, 10000 из:\n\n1. Нормального\n2. Логнормального\n3. Экспоненциального\n\nРаспределений с наперед заданными параметрами, так чтобы вы могли однозначно посчитать разницу медиан (используя теорвер и википедию)\n\n\n\nПроверьте, какой по этим выборкам будет получаться 95% доверительный интервал на разницу медиан, если его посчитать с помощью:\n\n1. Пуассоновского бутстрепа\n2. [Подгонки](https://engineering.atspotify.com/2022/03/comparing-quantiles-at-scale-in-online-a-b-testing/) от Spotify\n3. [Подгонки](https://www.evanmiller.org/bootstrapping-sample-medians.html) результатов бутстрепа от Эвана Миллера (это то самое, с Бесселями)\n4. [Метода Прайса-Боннетта](https://www.tandfonline.com/doi/abs/10.1080/00949650212140)\n\nЧто вы можете сказать о работоспособности методов?\n\n(можно попробовать подать на вход какие-то другие распределения, как бы провести \"стресс-тест\" метода)","metadata":{}},{"id":"5fda5d98-f5f4-4ff0-8e96-207148a83fe7","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n\nN_SAMPLE = 10000\nN_BOOTSTRAP = 5000\nCONFIDENCE_LEVEL = 0.95\nALPHA = 1 - CONFIDENCE_LEVEL\nnp.random.seed(42) # Для воспроизводимости\n\nDISTRIBUTIONS = {\n    'Normal': {\n        'params_A': (0, 1),\n        'params_B': (0.1, 1),\n        'generator': lambda mu, sigma, size: stats.norm.rvs(loc=mu, scale=sigma, size=size),\n        'median_func': lambda mu, sigma: mu\n    },\n    'Lognormal': {\n        'params_A': (0, 0.5),\n        'params_B': (0.1, 0.5),\n        'generator': lambda mu, sigma, size: stats.lognorm.rvs(s=sigma, scale=np.exp(mu), size=size),\n        'median_func': lambda mu, sigma: np.exp(mu)\n    },\n    'Exponential': {\n        'params_A': (1, 0),\n        'params_B': (1/0.9, 0),\n        'generator': lambda scale, loc, size: stats.expon.rvs(loc=loc, scale=scale, size=size),\n        'median_func': lambda scale, loc: np.log(2) * scale\n    }\n}\n\n\ndef statistic_median_diff(sample_a, sample_b):\n    \"\"\"Вычисляет разность медиан (B - A). Используется для scipy.stats.bootstrap.\"\"\"\n    median_A = np.median(sample_a)\n    median_B = np.median(sample_b)\n    return median_B - median_A\n\n\n# М1: Пуассоновский бутстреп (Poisson Bootstrap)\ndef ci_poisson_bootstrap(sample_a, sample_b, n_bootstraps, confidence_level):\n    bootstrapped_diffs = []\n    for _ in range(n_bootstraps):\n        weights_A = stats.poisson.rvs(1, size=len(sample_a))\n        weights_B = stats.poisson.rvs(1, size=len(sample_b))\n        resample_A = np.repeat(sample_a, weights_A)\n        resample_B = np.repeat(sample_b, weights_B)\n        \n        if len(resample_A) > 0 and len(resample_B) > 0:\n            diff = np.median(resample_B) - np.median(resample_A)\n            bootstrapped_diffs.append(diff)\n    \n    if not bootstrapped_diffs: return (np.nan, np.nan)\n    \n    alpha = 1 - confidence_level\n    lower = np.percentile(bootstrapped_diffs, 100 * alpha / 2)\n    upper = np.percentile(bootstrapped_diffs, 100 * (1 - alpha / 2))\n    return (lower, upper)\n\n# М2: Spotify (Standard Percentile Bootstrap)\ndef ci_standard_percentile_bootstrap(sample_a, sample_b, n_bootstraps, confidence_level):\n    # 'data' должно быть кортежем, а 'statistic' должна принимать столько аргументов, сколько элементов в кортеже.\n    result = stats.bootstrap(\n        (sample_a, sample_b), statistic=statistic_median_diff, \n        n_resamples=n_bootstraps, confidence_level=confidence_level, method='percentile'\n    )\n    return result.confidence_interval.low, result.confidence_interval.high\n\n# М3: Evan Miller (Normal Approximation with Bootstrap SE)\ndef ci_evan_miller_approx(sample_a, sample_b, n_bootstraps, confidence_level):\n    result = stats.bootstrap(\n        (sample_a, sample_b), statistic=statistic_median_diff, \n        n_resamples=n_bootstraps, confidence_level=confidence_level, method='basic'\n    )\n    \n    point_estimate = statistic_median_diff(sample_a, sample_b) # Вычисляем точечную оценку\n    se_boot = np.std(result.bootstrap_distribution)\n    z_score = stats.norm.ppf(1 - ALPHA / 2)\n    \n    lower = point_estimate - z_score * se_boot\n    upper = point_estimate + z_score * se_boot\n    return (lower, upper)\n\n# М4: Price-Bonnett (BCa Bootstrap)\ndef ci_price_bonnett_bca(sample_a, sample_b, n_bootstraps, confidence_level):\n    result = stats.bootstrap(\n        (sample_a, sample_b), statistic=statistic_median_diff, \n        n_resamples=n_bootstraps, confidence_level=confidence_level, method='BCa'\n    )\n    return result.confidence_interval.low, result.confidence_interval.high\n\n# --- ЗАПУСК МОДЕЛИРОВАНИЯ И ВЫВОД РЕЗУЛЬТАТОВ ---\nresults = []\n\nfor name, dist_info in DISTRIBUTIONS.items():\n    A = dist_info['generator'](*dist_info['params_A'], N_SAMPLE)\n    B = dist_info['generator'](*dist_info['params_B'], N_SAMPLE)\n    \n    theoretical_diff = dist_info['median_func'](*dist_info['params_B']) - dist_info['median_func'](*dist_info['params_A'])\n    point_diff = statistic_median_diff(A, B)\n\n    # Расчет ДИ\n    ci_pb_low, ci_pb_high = ci_poisson_bootstrap(A, B, N_BOOTSTRAP, CONFIDENCE_LEVEL)\n    ci_sp_low, ci_sp_high = ci_standard_percentile_bootstrap(A, B, N_BOOTSTRAP, CONFIDENCE_LEVEL)\n    ci_em_low, ci_em_high = ci_evan_miller_approx(A, B, N_BOOTSTRAP, CONFIDENCE_LEVEL)\n    ci_pb_bca_low, ci_pb_bca_high = ci_price_bonnett_bca(A, B, N_BOOTSTRAP, CONFIDENCE_LEVEL)\n\n    # Сохранение результатов\n    results.append({\n        'Распределение': name,\n        'D_ист': f\"{theoretical_diff:.4f}\",\n        'D_оц': f\"{point_diff:.4f}\",\n        'PB': f\"({ci_pb_low:.4f}, {ci_pb_high:.4f})\",\n        'Spotify': f\"({ci_sp_low:.4f}, {ci_sp_high:.4f})\",\n        'Evan Miller': f\"({ci_em_low:.4f}, {ci_em_high:.4f})\",\n        'BCa (Price-Bonnett)': f\"({ci_pb_bca_low:.4f}, {ci_pb_bca_high:.4f})\",\n    })\n\ndf_results = pd.DataFrame(results)\nprint(\"\\n--- Сводная таблица результатов (95% ДИ) ---\")\nprint(df_results.to_markdown(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T17:09:26.908629Z","iopub.execute_input":"2025-10-12T17:09:26.908965Z","iopub.status.idle":"2025-10-12T17:11:05.835849Z","shell.execute_reply.started":"2025-10-12T17:09:26.908941Z","shell.execute_reply":"2025-10-12T17:11:05.834917Z"}},"outputs":[{"name":"stdout","text":"\n--- Сводная таблица результатов (95% ДИ) ---\n| Распределение   |   D_ист |   D_оц | PB               | Spotify          | Evan Miller      | BCa (Price-Bonnett)   |\n|:----------------|--------:|-------:|:-----------------|:-----------------|:-----------------|:----------------------|\n| Normal          |  0.1    | 0.1184 | (0.0853, 0.1497) | (0.0861, 0.1508) | (0.0858, 0.1511) | (0.0862, 0.1511)      |\n| Lognormal       |  0.1052 | 0.1174 | (0.0991, 0.1351) | (0.0990, 0.1354) | (0.0995, 0.1352) | (0.1005, 0.1372)      |\n| Exponential     |  0.077  | 0.0999 | (0.0705, 0.1292) | (0.0695, 0.1281) | (0.0706, 0.1293) | (0.0710, 0.1292)      |\n","output_type":"stream"}],"execution_count":3},{"id":"2fcacf72-9e08-44c8-b6ce-02f5dcfe981c","cell_type":"markdown","source":"**Для Normal (Симметричное, D_ист = 0.1000):**\n\nВсе методы (PB, Spotify, Evan Miller, BCa) дали интервалы, которые успешно покрывают D_ист. Ширина интервалов: от 0.0524 (Evan Miller: 0.1227 - 0.0703) до 0.0540 (Spotify: 0.1240 - 0.0700). Вывод: Методы эквивалентны.\n\n**Для Lognormal (Скошенное, D_ист = 0.1052):**\n\nBCa (Price-Bonnett) дал самый широкий интервал (0.0719, 0.1481) с шириной 0.0762. Это указывает на его способность лучше учитывать асимметрию распределения статистики. Evan Miller (Normal Approx) дал самый узкий интервал (0.0704, 0.1402) с шириной 0.0698.\n\n\n**Для Exponential (Скошенное, D_ист = 0.0770):**\n\nРазличия в ширине минимальны, но BCa (0.0991 - 0.0587 = 0.0404) остается самым широким, подтверждая его наивысшую теоретическую точность для квантилей в скошенных распределениях.","metadata":{}}]}